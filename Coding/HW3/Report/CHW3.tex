\documentclass[12pt,a4paper]{article}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{booktabs}
\usepackage{url}
\usepackage{float}
\usepackage{caption}

% بسته زی‌پرشین باید آخرین بسته‌ای باشد که فراخوانی می‌شود
\usepackage{xepersian}
\settextfont[Scale=1.1]{Amiri}
\setlatintextfont{Times New Roman}

\title{\textbf{تکلیف کامپیوتری سوم - درس شناسایی الگو}}
\author{وحید ملکی \\ شماره دانشجویی: 40313004}
\date{\today}

\begin{document}
	
	\maketitle
	
	\begin{abstract}
		در این تکلیف، هدف پیاده‌سازی و بررسی عملکرد الگوریتم $\text{EM}$ (Expectation-Maximization) بر روی مدل ترکیبی گوسی ($\text{GMM}$) است. مسئله‌ی مورد مطالعه، $\text{Annulus Problem}$ (مسئله‌ی حلقه) است که در آن نقاط داده‌ای به‌صورت یکنواخت در ناحیه‌ی بین دو دایره قرار دارند. با استفاده از $\text{GMM}$ با $C=30$ مؤلفه‌ی گوسی و ماتریس‌های کوواریانس قطری، پارامترهای مدل تخمین زده شده و همگرایی مدل از طریق رسم کانتورهای بیضی در تکرارهای مختلف و بررسی نمودار $\text{Log-Likelihood}$ تحلیل می‌شود.
	\end{abstract}
	
	\newpage
	
	\section{مقدمه و شرح مسئله}
	
	مسئله‌ی اصلی در این تمرین، یافتن توزیع احتمال (و پارامترهای آن) برای مجموعه‌ای از داده‌های دوبعدی است که از یک توزیع ساده‌ی نرمال تولید نشده‌اند، بلکه ساختاری پیچیده (شکل حلقوی) دارند. مدل ترکیبی گوسی ($\text{Gaussian Mixture Model}$ یا $\text{GMM}$) یک راهکار قوی برای مدل‌سازی این‌گونه توزیع‌های پیچیده است.
	
	\subsection{تولید مجموعه‌ی داده ($\text{Annulus Data}$)}
	مجموعه‌ی داده شامل $N=900$ نقطه دوبعدی است که به‌صورت یکنواخت در ناحیه‌ی بین دو دایره (حلقه) با شعاع داخلی $r_{\text{inner}}=1$ و شعاع خارجی $r_{\text{outer}}=2$ قرار دارند.
	برای تضمین یکنواختی نقاط در مساحت حلقه (و نه صرفاً در شعاع)، از روش تولید شعاع تصادفی با توزیع مناسب استفاده شده است. این روش شامل تولید $r$ از توزیع یکنواخت برای مربع شعاع، یعنی $r^2 \sim \text{Uniform}(r_{\text{inner}}^2, r_{\text{outer}}^2)$ است. زاویه‌ها نیز به‌صورت یکنواخت در بازه‌ی $(0, 2\pi)$ تولید شده‌اند.
	
	\subsection{تعریف مدل ترکیبی گوسی}
	مدل $\text{GMM}$ با $C=30$ مؤلفه‌ی گوسی دوبعدی تعریف شده است. پارامترهای مدل شامل موارد زیر هستند:
	\begin{itemize}
		\item وزن‌های ترکیبی: $\pi_{k}$، به‌طوری که $\sum_{k=1}^{C} \pi_{k}=1$.
		\item میانگین‌ها: $\mu_{k} \in \mathbb{R}^{2}$.
		\item ماتریس‌های کوواریانس: $\Sigma_{k}$ که ماتریس‌های قطری $2 \times 2$ هستند.
	\end{itemize}
	
	\section{پیاده‌سازی الگوریتم $\text{EM}$ و منطق حل}
	
	از آنجایی که برچسب هر نقطه (اینکه از کدام مؤلفه تولید شده) مجهول است ($\text{Missing data}$)، از الگوریتم تکراری $\text{EM}$ (Expectation-Maximization) برای تخمین پارامترهای $\theta = \{\pi_k, \mu_k, \Sigma_k\}_{k=1}^C$ استفاده می‌شود.
	
	\subsection{مقداردهی اولیه ($\text{Initialization}$)}
	مقداردهی اولیه مطابق دستورالعمل تمرین انجام شده است:
	\begin{itemize}
		\item \textbf{میانگین‌ها ($\mu_k$):} $30$ نقطه‌ی داده به‌صورت تصادفی از مجموعه‌ی $X$ انتخاب شدند.
		\item \textbf{وزن‌ها ($\pi_k$):} همگی به‌صورت مساوی $\pi_{k}=1/C = 1/30$ تنظیم شدند.
		\item \textbf{کوواریانس‌ها ($\Sigma_k$):} ماتریس‌های قطری با واریانس نسبتاً بزرگ (براساس واریانس کل داده‌ها) مقداردهی شدند تا از بروز پدیده‌ی $\text{singularity}$ جلوگیری شود.
	\end{itemize}
	
	\subsection{مراحل تکراری الگوریتم}
	الگوریتم $\text{EM}$ در هر تکرار دو گام اصلی را اجرا می‌کند:
	\begin{enumerate}
		\item \textbf{گام امید ($\text{E-Step}$):}
		در این گام، مسئولیت‌ها ($\text{Responsibilities}$) یا $\gamma_{ik}$ محاسبه می‌شود. این مسئولیت نشان‌دهنده‌ی احتمال تعلق نقطه‌ی $x_i$ به مؤلفه‌ی گوسی $k$-اُم، با توجه به پارامترهای فعلی ($\theta^{\text{old}}$) است.
		$$
		\gamma_{ik} = P(z_i=k | x_i, \theta^{\text{old}}) = \frac{\pi_k \mathcal{N}(x_i | \mu_k, \Sigma_k)}{\sum_{j=1}^{C} \pi_j \mathcal{N}(x_i | \mu_j, \Sigma_j)}
		$$
		\item \textbf{گام بیشینه‌سازی ($\text{M-Step}$):}
		با استفاده از مسئولیت‌های $\gamma_{ik}$ محاسبه شده در گام $\text{E}$، پارامترهای جدید ($\theta^{\text{new}}$) به‌گونه‌ای محاسبه می‌شوند که امید ریاضی لگاریتم شباهت (که در $\text{E-Step}$ محاسبه شده) بیشینه شود.
		\begin{itemize}
			\item تعداد مؤثر نقاط اختصاص داده شده به مؤلفه‌ی $k$: $N_k = \sum_{i=1}^N \gamma_{ik}$
			\item به‌روزرسانی میانگین: $\mu_k^{\text{new}} = \frac{1}{N_k} \sum_{i=1}^N \gamma_{ik} x_i$
			\item به‌روزرسانی کوواریانس: $\Sigma_k^{\text{new}} = \frac{1}{N_k} \sum_{i=1}^N \gamma_{ik} (x_i - \mu_k^{\text{new}}) (x_i - \mu_k^{\text{new}})^T$
			\item به‌روزرسانی وزن: $\pi_k^{\text{new}} = \frac{N_k}{N}$
		\end{itemize}
		در پیاده‌سازی، برای $\Sigma_k^{\text{new}}$ تنها عناصر روی قطر اصلی نگهداری شدند تا شرط ماتریس کوواریانس \textbf{قطری} حفظ شود.
	\end{enumerate}
	
	\section{تحلیل خروجی‌ها}
	
	خروجی‌های کد پایتون شامل سه بخش اصلی زیر است:
	
	\subsection{نمودار داده‌های تولید شده ($\text{Generated Annulus Dataset}$)}
	
	\begin{figure}[H]
		\centering
		% استفاده از آکولاد برای پشتیبانی از فاصله در نام فایل
		\includegraphics[width=0.6\textwidth]{{Generated Annulus Dataset}.png}
		\caption{نقاط داده‌ای $\text{Annulus}$ تولید شده. یکنواختی سطحی حفظ شده و شعاع داخلی $1$ و خارجی $2$ به‌خوبی قابل مشاهده است.}
	\end{figure}
	
	\textbf{تحلیل:} همان‌طور که در شکل مشاهده می‌شود، $N=900$ نقطه به‌صورت یکنواخت در فضای حلقوی بین $r=1$ و $r=2$ توزیع شده‌اند. این نشان می‌دهد که منطق تولید داده با استفاده از تبدیل تصادفی $r \sim \sqrt{U}$ به درستی اعمال شده است.
	
	\subsection{کانتورهای گوسی در تکرارهای مختلف}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.9\textwidth]{{GMM Plot}.png}
		\caption{بیضی‌های $\text{GMM}$ در تکرارهای $0، 25، 50، 75، 275$ و $300$. نقاط آبی داده‌ها و بیضی‌های قرمز، کانتور $2\sigma$ هر مؤلفه گوسی را نشان می‌دهند.}
	\end{figure}
	
	\textbf{تحلیل همگرایی:}
	\begin{itemize}
		\item \textbf{تکرار $0$ ($\text{Initialization}$):} بیضی‌ها به‌صورت اولیه بزرگ و متمرکز در نزدیکی مرکز هستند و تقریباً همه‌ی ناحیه‌ی داخلی را پوشش می‌دهند، که نتیجه‌ی مقداردهی اولیه با واریانس بزرگ است.
		\item \textbf{تکرار $25$ و $50$ ($\text{Early Convergence}$):} مؤلفه‌ها به‌سرعت از مرکز فاصله گرفته و شروع به قرارگیری در ناحیه‌ی متراکم داده‌ها (حلقه) می‌کنند. بیضی‌ها در این مرحله، داده‌ها را به‌صورت نودولار (گره‌ای) پوشش می‌دهند.
		\item \textbf{تکرار $75$ تا $300$ ($\text{Final Convergence}$):} با پیشرفت الگوریتم، $30$ مؤلفه‌ی گوسی به‌صورت منظم در امتداد حلقه پخش می‌شوند و هر مؤلفه مسئول مدل‌سازی بخشی کوچک از انحنای حلقه است. در تکرارهای $275$ و $300$، تفاوت بسیار اندک است که نشان‌دهنده‌ی رسیدن مدل به یک بیشینه‌ی محلی از شباهت ($\text{Maximum Local Likelihood}$) است. این رفتار مطلوب و مطابق با انتظار مسئله است.
	\end{itemize}
	
	\subsection{نمودار $\text{Log-Likelihood}$ در مقابل تکرار}
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{{Log-Likelihood vs. Iteration}.png}
		\caption{نمودار $\text{Log-Likelihood}$ در برابر تکرارها.}
	\end{figure}
	
	\textbf{تحلیل:}
	\begin{itemize}
		\item \textbf{صعود مونوتون:} همان‌طور که در تئوری الگوریتم $\text{EM}$ مورد انتظار است، مقدار $\text{Log-Likelihood}$ در هر تکرار افزایش می‌یابد (صعودی یکنواخت یا $\text{Monotonic Increase}$).
		\item \textbf{همگرایی:} شیب افزایش در تکرارهای اولیه (تا حدود تکرار $100$) بسیار تند است و پس از آن کاهش می‌یابد. نمودار در حوالی تکرار $200$ تا $250$ عملاً \textbf{اشباع ($\text{Saturation}$)} شده و مقدار آن به نزدیکی مقدار نهایی ($\approx -2005$) می‌رسد. این همگرایی قوی نشان‌دهنده‌ی موفقیت الگوریتم در تخمین پارامترها و رسیدن به بیشینه‌ی محلی شباهت است.
	\end{itemize}
	
\end{document}