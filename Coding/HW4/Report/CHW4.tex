\documentclass[12pt,a4paper]{article}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{booktabs}
\usepackage{url}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption} % برای قرار دادن تصاویر کنار هم
\usepackage{listings}
\usepackage{xcolor}

\usepackage{xepersian}
\settextfont[Scale=1.1]{Amiri}
\setlatintextfont{Times New Roman}

\title{\textbf{تکلیف کامپیوتری چهارم - درس شناسایی الگو}}
\author{وحید ملکی \\ شماره دانشجویی: 40313004}
\date{\today}

\begin{document}
	
	\maketitle
	
	\section{مقدمه}
	در این تمرین، هدف پیاده‌سازی و ارزیابی الگوریتم ماشین بردار پشتیبان (\lr{Support Vector Machine - SVM}) برای طبقه‌بندی تصاویر مجموعه داده \lr{Fashion-MNIST} است. این کار در دو بخش انجام شده است: ابتدا پیاده‌سازی الگوریتم از پایه با استفاده از کتابخانه \lr{NumPy} و ریاضیات مربوط به مسئله دوگان (\lr{Dual Problem})، و سپس پیاده‌سازی با استفاده از کتابخانه \lr{Scikit-Learn} جهت اعتبارسنجی نتایج. در نهایت تاثیر پارامترهای مختلف مانند نوع کرنل و ضریب جریمه ($C$) بر دقت مدل بررسی شده است.
	
	\section{بخش اول: پیاده‌سازی SVM از پایه (NumPy)}
	
	\subsection{آماده‌سازی داده‌ها}
	مجموعه داده \lr{Fashion-MNIST} شامل تصاویر سیاه‌وسفی $28 \times 28$ پیکسل از ۱۰ کلاس مختلف پوشاک است. در این پیاده‌سازی، مراحل زیر برای آماده‌سازی داده‌ها طی شد:
	\begin{enumerate}
		\item \textbf{بارگذاری و انتخاب زیرمجموعه:} به دلیل حجم بالای داده‌ها و هزینه محاسباتی بالای آموزش SVM (که با تعداد نمونه‌ها رابطه نمایی دارد)، پس از بارگذاری کل داده‌ها، آن‌ها به صورت تصادفی مخلوط (\lr{Shuffle}) شدند و یک زیرمجموعه شامل $2000$ نمونه انتخاب گردید.
		\item \textbf{تقسیم داده‌ها:} داده‌ها با استفاده از روش \lr{Stratified Split} به دو بخش آموزش ($80\%$) و آزمون ($20\%$) تقسیم شدند تا توزیع کلاس‌ها در هر دو مجموعه حفظ شود.
		\item \textbf{نرمال‌سازی (\lr{Standardization}):} از آنجا که SVM یک الگوریتم مبتنی بر فاصله است، مقیاس ویژگی‌ها تاثیر زیادی بر عملکرد آن دارد. تمام ویژگی‌ها با کسر میانگین و تقسیم بر انحراف معیار نرمال شدند:
		\begin{equation}
			X_{scaled} = \frac{X - \mu}{\sigma + \epsilon}
		\end{equation}
		که در آن $\epsilon$ مقدار بسیار کوچکی برای جلوگیری از تقسیم بر صفر است.
	\end{enumerate}
	
	\subsection{ریاضیات و ساختار مدل}
	الگوریتم پیاده‌سازی شده بر اساس حل مسئله دوگان لاگرانژ (\lr{Lagrange Dual Problem}) بنا شده است. هدف پیدا کردن ضرایب $\alpha$ است که تابع هدف زیر را ماکزیمم کنند:
	
	\begin{equation}
		\max_{\alpha} \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y_i y_j K(x_i, x_j)
	\end{equation}
	
	تحت قیود:
	\begin{equation}
		0 \le \alpha_i \le C, \quad \forall i
	\end{equation}
	\begin{equation}
		\sum_{i=1}^{m} \alpha_i y_i = 0
	\end{equation}
	
	در کد نوشته شده، برای حل این مسئله بهینه‌سازی از روش گرادیان صعودی (\lr{Gradient Ascent}) استفاده شده است. گرادیان تابع هدف نسبت به بردار $\alpha$ به صورت زیر محاسبه می‌شود:
	\begin{equation}
		\nabla L(\alpha) = \mathbf{1} - y \odot (K \cdot (\alpha \odot y))
	\end{equation}
	که در آن $\odot$ ضرب درایه‌ای و $K$ ماتریس کرنل است. در هر تکرار، مقادیر $\alpha$ به‌روزرسانی شده و سپس جهت ارضای قید $0 \le \alpha \le C$، مقادیر آن به بازه $[0, C]$ محدود (\lr{Clip}) می‌شوند.
	
	\subsection{توابع کرنل (\lr{Kernel Functions})}
	دو نوع کرنل در تابع \lr{\_kernel} پیاده‌سازی شد:
	\begin{itemize}
		\item \textbf{کرنل خطی (\lr{Linear}):}
		\begin{equation}
			K(x_1, x_2) = x_1 \cdot x_2^T
		\end{equation}
		\item \textbf{کرنل \lr{RBF} (توزیع گاوسی):}
		\begin{equation}
			K(x_1, x_2) = \exp(-\gamma ||x_1 - x_2||^2)
		\end{equation}
		که پارامتر $\gamma$ به صورت پیش‌فرض برابر با $\frac{1}{n\_features}$ در نظر گرفته شد.
	\end{itemize}
	
	\subsection{محاسبه بایاس ($b$) و تابع تصمیم}
	پس از همگرایی گرادیان، بردارهای پشتیبان (\lr{Support Vectors}) شناسایی شدند (نقاطی که $\alpha_i > 0$). مقدار بایاس $b$ با میانگین‌گیری روی بردارهای پشتیبان محاسبه شد:
	\begin{equation}
		b = \frac{1}{N_{SV}} \sum_{s \in SV} (y_s - \sum_{i} \alpha_i y_i K(x_i, x_s))
	\end{equation}
	تابع تصمیم نهایی برای یک داده ورودی جدید $x$ برابر است با:
	\begin{equation}
		f(x) = \text{sign}(\sum_{i} \alpha_i y_i K(x_i, x) + b)
	\end{equation}
	
	\subsection{استراتژی چندکلاسه (\lr{One-vs-Rest})}
	از آنجا که SVM ذاتا یک طبقه‌بند دودویی است، برای مجموعه داده \lr{Fashion-MNIST} که دارای ۱۰ کلاس است، از استراتژی «یکی در برابر همه» (\lr{One-vs-Rest}) استفاده شد. به این صورت که ۱۰ مدل مجزا آموزش داده شدند؛ در مدل $k$-ام، نمونه‌های کلاس $k$ برچسب $+1$ و سایر نمونه‌ها برچسب $-1$ گرفتند. برای پیش‌بینی نهایی، کلاسی انتخاب شد که بیشترین مقدار تابع تصمیم (امتیاز) را تولید کرد.
	
	\section{ارزیابی و بهینه‌سازی پارامترها}
	جهت یافتن بهترین پارامترها، از روش اعتبارسنجی متقاطع ۱۰ لایه (\lr{10-Fold Cross-Validation}) روی داده‌های آموزشی استفاده شد. مقادیر مختلف پارامتر جریمه $C \in \{0.01, 0.1, 1, 10, 100\}$ برای هر دو کرنل خطی و \lr{RBF} مورد ارزیابی قرار گرفتند.
	
	\subsection{تحلیل نتایج پیاده‌سازی \lr{Scratch}}
	همانطور که در نمودار سمت راست شکل \ref{fig:accuracy_comparison} مشاهده می‌شود:
	\begin{itemize}
		\item \textbf{کرنل خطی:} عملکرد بسیار ضعیفی داشت (دقت حدود $13\%$ تا $16\%$). این نشان می‌دهد که داده‌های تصویر پوشاک در فضای ویژگی اصلی به صورت خطی تفکیک‌پذیر نیستند.
		\item \textbf{کرنل \lr{RBF}:} عملکرد بسیار بهتری نشان داد. با افزایش مقدار $C$، دقت مدل افزایش یافت و در مقادیر $C=10$ و $C=100$ به ثبات رسید (حدود $83\%$). این نشان می‌دهد که مدل با مرزهای غیرخطی و جریمه سخت‌گیرانه‌تر برای خطاهای طبقه‌بندی، بهتر عمل می‌کند.
	\end{itemize}
	
	بهترین پارامترهای یافت شده عبارتند از:
	\begin{itemize}
		\item \textbf{کرنل:} \lr{rbf}
		\item \textbf{مقدار $C$:} $10$
		\item \textbf{دقت نهایی روی داده تست:} $82.62\%$
	\end{itemize}
	
	\section{بخش دوم: پیاده‌سازی با Scikit-Learn}
	برای مقایسه، همین مراحل با استفاده از کلاس‌های \lr{SVC}، \lr{OneVsRestClassifier} و \lr{GridSearchCV} از کتابخانه \lr{Scikit-Learn} انجام شد.
	نتایج حاصل از \lr{Scikit-Learn} تطابق بسیار بالایی با پیاده‌سازی دستی داشت که صحت عملکرد کد نوشته شده را تایید می‌کند. بهترین مدل در این حالت نیز کرنل \lr{RBF} با $C=10$ بود که دقتی برابر با $82.50\%$ روی داده‌های تست نهایی ثبت کرد.
	
	\section{تحلیل نمودارها و مقایسه}
	
	\subsection{مقایسه روند دقت بر حسب $C$}
	در شکل \ref{fig:accuracy_comparison} روند تغییرات دقت بر اساس پارامتر $C$ برای هر دو پیاده‌سازی نمایش داده شده است.
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.48\textwidth}
			\centering
			 \includegraphics[width=\linewidth]{numpy_results.png}

			\caption{پیاده‌سازی دستی (NumPy)}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.48\textwidth}
			\centering
			 \includegraphics[width=\linewidth]{sklearn_results.png}

			\caption{پیاده‌سازی Scikit-Learn}
		\end{subfigure}
		\caption{نمودار تغییرات دقت میانگین (Cross-Validation) بر حسب مقدار $C$ برای کرنل‌های خطی و RBF.}
		\label{fig:accuracy_comparison}
	\end{figure}
	
	در هر دو نمودار، کرنل \lr{RBF} (خط نارنجی در نمودار دستی و خط آبی در نمودار \lr{Sklearn}) با افزایش $C$ بهبود یافته و سپس اشباع می‌شود. تفاوت جزیی در نمودار خطی ممکن است ناشی از تفاوت در روش‌های بهینه‌سازی داخلی (\lr{Gradient Ascent} ساده در برابر \lr{LibSVM} بهینه شده) باشد.
	
	\subsection{ماتریس درهم‌ریختگی (\lr{Confusion Matrix})}
	شکل \ref{fig:confusion_matrices} ماتریس درهم‌ریختگی مدل بهینه را برای هر دو روش نشان می‌دهد.
	
	\begin{figure}[H]
		\centering
		\begin{subfigure}{0.48\textwidth}
			\centering
			 \includegraphics[width=\linewidth]{confusion_matrix_numpy_results.png}

			\caption{پیاده‌سازی دستی}
		\end{subfigure}
		\hfill
		\begin{subfigure}{0.48\textwidth}
			\centering
			 \includegraphics[width=\linewidth]{confusion_matrix_sklearn_results.png}

			\caption{پیاده‌سازی Scikit-Learn}
		\end{subfigure}
		\caption{ماتریس درهم‌ریختگی برای بهترین مدل انتخاب شده روی داده‌های تست.}
		\label{fig:confusion_matrices}
	\end{figure}
	
	تحلیل ماتریس درهم‌ریختگی نکات زیر را روشن می‌کند:
	\begin{itemize}
		\item کلاس ۶ (\lr{Shirt}) بیشترین نرخ خطا را دارد و اغلب با کلاس ۰ (\lr{T-shirt/Top})، کلاس ۲ (\lr{Pullover}) و کلاس ۴ (\lr{Coat}) اشتباه گرفته می‌شود. این خطا به دلیل شباهت ظاهری زیاد این دسته از پوشاک قابل انتظار است.
		\item کلاس ۱ (\lr{Trouser}) و کلاس ۹ (\lr{Ankle boot}) بالاترین دقت تشخیص را دارند (حدود $94\%$ تا $98\%$) که نشان‌دهنده تمایز واضح ویژگی‌های آن‌ها نسبت به سایر کلاس‌هاست.
		\item تطابق بالای مقادیر قطر اصلی در هر دو ماتریس نشان می‌دهد که پیاده‌سازی دستی به درستی توانسته است الگوهای موجود را مشابه با پیاده‌سازی استاندارد \lr{Scikit-Learn} فرا بگیرد.
	\end{itemize}
	
	\section{نتیجه‌گیری}
	در این تمرین، الگوریتم SVM با موفقیت پیاده‌سازی شد. نتایج نشان داد که برای داده‌های تصویری \lr{Fashion-MNIST}، استفاده از کرنل غیرخطی \lr{RBF} ضروری است. مدل دستی توانست به دقتی معادل $82.62\%$ دست یابد که با دقت $82.50\%$ حاصل از کتابخانه \lr{Scikit-Learn} قابل رقابت است.
	
\end{document}