\documentclass[12pt,a4paper]{article}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\geometry{margin=2.5cm}
\usepackage{booktabs}
\usepackage{url}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{xcolor}

\usepackage{xepersian}
\settextfont[Scale=1.1]{Amiri}
\setlatintextfont{Times New Roman}

\title{\textbf{تکلیف کامپیوتری پنجم - درس بازشناسی آماری الگو}}
\author{نام دانشجو \\ شماره دانشجویی: 40313004}
\date{\today}

\begin{document}
	
	\maketitle
	
	\section{مقدمه}
	در این تمرین، هدف طراحی، پیاده‌سازی و ارزیابی یک شبکه عصبی کانولوشنی (\lr{CNN}) برای طبقه‌بندی مجموعه داده \lr{KMNIST} است. برخلاف شبکه‌های تمام متصل (\lr{MLP}) که با برداری‌کردن تصویر، اطلاعات مکانی پیکسل‌ها را از بین می‌برند، شبکه‌های \lr{CNN} با حفظ ساختار دوبعدی تصویر و استفاده از فیلترها، ویژگی‌های بصری را به شکل مؤثرتری استخراج می‌کنند. در ادامه مراحل آماده‌سازی داده، طراحی معماری شبکه، و ارزیابی مدل با دو روش \lr{Hold-out} و \lr{K-Fold Cross Validation} تشریح می‌گردد.
	
	\section{بارگذاری و پیش‌پردازش داده‌ها}
	برای اجرای این پروژه از فریم‌ورک \lr{PyTorch} استفاده شده است. مجموعه داده \lr{KMNIST} شامل تصاویر حروف ژاپنی کوزوشیجی است.
	
	\subsection{وضعیت مجموعه داده}
	طبق صورت تمرین، از کل مجموعه داده آموزشی شامل $60000$ تصویر برای آموزش و $10000$ تصویر برای تست استفاده شد.
	بررسی توازن کلاس‌ها نشان داد که داده‌ها کاملاً متوازن (\lr{Balanced}) هستند:
	\begin{itemize}
		\item تعداد کل کلاس‌ها: $10$ کلاس
		\item تعداد نمونه در هر کلاس (در داده‌های آموزش): دقیقاً $6000$ نمونه
	\end{itemize}
	بنابراین نیازی به تکنیک‌های نمونه‌برداری جهت رفع عدم توازن وجود نداشت.
	
	\subsection{پیش‌پردازش}
	عملیات زیر روی تصاویر اعمال شد:
	\begin{itemize}
		\item \textbf{تبدیل به تنسور (\lr{To Tensor}):} تبدیل داده‌ها به فرمت قابل پردازش و نوع داده \lr{Float}.
		\item \textbf{نرمال‌سازی (\lr{Normalization}):} مقادیر پیکسل‌ها با میانگین $0.5$ و انحراف معیار $0.5$ نرمال‌سازی شدند تا دامنه مقادیر به بازه $[-1, 1]$ منتقل شود. این عمل باعث همگرایی سریع‌تر گرادیان‌ها در طول آموزش می‌شود.
	\end{itemize}
	
	\section{طراحی معماری شبکه عصبی (\lr{CNN})}
	مدل پیشنهادی (\lr{KMNIST\_CNN}) شامل سه بلوک استخراج ویژگی و یک طبقه بندی‌کننده نهایی است. طراحی این معماری با هدف کاهش \lr{Overfitting} و استخراج ویژگی‌های سلسله‌مراتبی انجام شده است. جزئیات لایه‌ها به شرح زیر است:
	
	\begin{enumerate}
		\item \textbf{لایه‌های کانولوشن (\lr{Conv2d}):} از سه لایه کانولوشن با کرنل سایز $3\times3$ و پدینگ $1$ استفاده شد. تعداد فیلترها به ترتیب $32$، $64$ و $128$ در نظر گرفته شد تا با عمیق‌تر شدن شبکه، ویژگی‌های پیچیده‌تری استخراج شود.
		\item \textbf{نرمال‌سازی دسته‌ای (\lr{Batch Normalization}):} پس از هر لایه کانولوشن، از لایه \lr{BatchNorm2d} استفاده شد. این لایه با نرمال‌کردن خروجی‌های هر لایه میانی، وابستگی به مقداردهی اولیه را کاهش داده و سرعت آموزش را افزایش می‌دهد.
		\item \textbf{تابع فعال‌ساز (\lr{Activation Function}):} در تمامی لایه‌ها از تابع غیرخطی \lr{ReLU} استفاده شده است.
		\item \textbf{لایه ادغام (\lr{Max Pooling}):} جهت کاهش ابعاد مکانی و حجم محاسبات، از \lr{MaxPooling} با ابعاد $2\times2$ بعد از هر بلوک کانولوشن استفاده شد.
		\item \textbf{طبقه‌بندی‌کننده (\lr{Classifier}):} خروجی لایه‌های کانولوشن ابتدا \lr{Flatten} شده و وارد یک شبکه تمام متصل می‌شود. جهت جلوگیری از بیش‌برازش، یک لایه \lr{Dropout} با نرخ $0.5$ قبل از لایه خروجی قرار داده شد.
	\end{enumerate}
	
	\section{تنظیم فرآیند آموزش}
	پارامترهای مربوط به آموزش مدل به شرح زیر تنظیم شدند:
	\begin{itemize}
		\item \textbf{بهینه‌ساز (\lr{Optimizer}):} الگوریتم \lr{Adam} به دلیل تطبیق‌پذیری نرخ یادگیری انتخاب شد.
		\item \textbf{تابع هزینه (\lr{Loss Function}):} تابع \lr{CrossEntropyLoss} که مناسب مسائل طبقه‌بندی چندکلاسه است.
		\item \textbf{سخت‌افزار:} کلیه محاسبات بر روی \lr{GPU} (در محیط \lr{Google Colab/Local CUDA}) اجرا شد.
	\end{itemize}
	
	\section{ارزیابی مدل با روش \lr{Hold-out}}
	در این روش، داده‌های آموزشی به دو بخش تقسیم شدند:
	\begin{itemize}
		\item \textbf{داده‌های آموزش (\lr{Train}):} $80\%$ از داده‌ها ($48000$ نمونه).
		\item \textbf{داده‌های اعتبارسنجی (\lr{Validation}):} $20\%$ از داده‌ها ($12000$ نمونه).
	\end{itemize}
	مجموعه داده تست ($10000$ نمونه) کاملاً ایزوله نگه داشته شد. مدل با نرخ یادگیری $0.001$ و اندازه دسته (\lr{Batch Size}) برابر با $64$ به مدت $15$ دور (\lr{Epoch}) آموزش دید.
	
	\subsection{تحلیل نمودارهای آموزش}
	در شکل \ref{fig:holdout_plots} نمودارهای خطا (\lr{Loss}) و دقت (\lr{Accuracy}) نمایش داده شده است. همگرایی نمودارهای آموزش و اعتبارسنجی و فاصله اندک بین آن‌ها نشان می‌دهد که مدل دچار بیش‌برازش شدید نشده و به خوبی تعمیم یافته است.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.95\textwidth]{Loss-Acc.png} 
		\caption{روند تغییرات خطا و دقت در طول $15$ دور آموزش (روش \lr{Hold-out})}
		\label{fig:holdout_plots}
	\end{figure}
	
	\subsection{نتایج ارزیابی نهایی (\lr{Test Data})}
	پس از پایان آموزش، مدل روی داده‌های تست ارزیابی شد. نتایج کلی عبارتند از:
	\begin{itemize}
		\item \textbf{دقت نهایی (\lr{Accuracy}):} برابر با $0.9656$ (یا $96.56\%$)
	\end{itemize}
	
	ماتریس درهم‌ریختگی در شکل \ref{fig:cm_holdout} و جزئیات دقت به تفکیک هر کلاس در جدول \ref{tab:per_class} آورده شده است.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{Hold-out CM.png} 
		\caption{ماتریس درهم‌ریختگی مدل روی داده‌های تست (روش \lr{Hold-out})}
		\label{fig:cm_holdout}
	\end{figure}
	
	\begin{table}[H]
		\centering
		\caption{دقت مدل به تفکیک کلاس‌ها (\lr{Per-class Accuracy})}
		\label{tab:per_class}
		\begin{tabular}{cccccc}
			\toprule
			\textbf{کلاس} & \textbf{دقت} & \textbf{کلاس} & \textbf{دقت} \\
			\midrule
			0 (\lr{o}) & $0.976$ & 5 (\lr{ha}) & $0.958$ \\
			1 (\lr{ki}) & $0.963$ & 6 (\lr{ma}) & $0.989$ \\
			2 (\lr{su}) & $0.919$ & 7 (\lr{ya}) & $0.967$ \\
			3 (\lr{tsu}) & $0.986$ & 8 (\lr{re}) & $0.981$ \\
			4 (\lr{na}) & $0.948$ & 9 (\lr{wo}) & $0.969$ \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	مشاهده می‌شود که کلاس شماره $2$ (\lr{su}) کمترین دقت را داشته است که با نگاه به ماتریس درهم‌ریختگی مشخص می‌شود بیشترین تداخل را با کلاس‌های دیگر (به خصوص کلاس $3$) داشته است.
	
	\section{ارزیابی مدل با روش \lr{K-Fold Cross Validation}}
	جهت یافتن بهترین هایپرپارامترها و ارزیابی دقیق‌تر، از روش \lr{10-Fold Cross Validation} استفاده شد. دو پیکربندی مختلف بررسی شد:
	\begin{enumerate}
		\item \lr{LR}=$0.001$ , \lr{Batch Size}=$128$ $\leftarrow$ میانگین دقت: $98.69\%$
		\item \lr{LR}=$0.0005$ , \lr{Batch Size}=$64$ $\leftarrow$ میانگین دقت: $98.83\%$
	\end{enumerate}
	
	\subsection{آموزش مدل نهایی}
	بر اساس نتایج بالا، پیکربندی دوم (نرخ یادگیری $0.0005$ و اندازه دسته $64$) به عنوان بهترین تنظیمات انتخاب شد. مدل نهایی با این تنظیمات روی \textbf{کل داده‌های آموزشی} ($60000$ نمونه) آموزش داده شد و سپس روی داده‌های تست ارزیابی گردید.
	
	\begin{itemize}
		\item \textbf{دقت نهایی تست (\lr{K-Fold Best Config}):} برابر با $0.9630$ (یا $96.30\%$)
	\end{itemize}
	
	ماتریس درهم‌ریختگی حاصل از این مدل در شکل \ref{fig:cm_kfold} نمایش داده شده است.
	
	\begin{figure}[H]
		\centering
		\includegraphics[width=0.7\textwidth]{K-Fold CM.png} 
		\caption{ماتریس درهم‌ریختگی مدل بهینه شده با روش \lr{K-Fold}}
		\label{fig:cm_kfold}
	\end{figure}
	
	\section{نتیجه‌گیری و مقایسه}
	در این تکلیف، شبکه \lr{CNN} با موفقیت پیاده‌سازی شد و در هر دو روش ارزیابی به دقتی بالاتر از $95\%$ دست یافت که شرط نمره امتیازی را برآورده می‌کند.
	
	\begin{table}[H]
		\centering
		\caption{مقایسه نهایی عملکرد روش‌ها روی داده‌های تست}
		\begin{tabular}{lcc}
			\toprule
			\textbf{روش ارزیابی} & \textbf{دقت نهایی تست} & \textbf{توضیحات} \\
			\midrule
			\lr{Hold-out} & $0.9656$ & آموزش سریع‌تر، استفاده از ۸۰ درصد داده \\
			\lr{K-Fold CV} & $0.9630$ & تنظیم دقیق‌تر پارامترها، استفاده از ۱۰۰ درصد داده \\
			\bottomrule
		\end{tabular}
	\end{table}
	
	هرچند دقت روش \lr{Hold-out} اندکی بالاتر به نظر می‌رسد، اما تفاوت ناچیز است ($0.2\%$) و نشان می‌دهد مدل پیشنهادی پایداری خوبی دارد. استفاده از \lr{Dropout} و \lr{Batch Normalization} نقش کلیدی در دستیابی به این دقت بالا و جلوگیری از \lr{Overfitting} داشته‌اند.
	
\end{document}